{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b302bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecmwf.opendata import Client\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../\")))  # set vscode notebook path for module imports\n",
    "\n",
    "from utils.tools import regions_from_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7100560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [i for i in range(0, 144, 3)] + [j for j in range(144, 361, 6)] #get all steps\n",
    "time = 0 if datetime.now().hour < 18 else 12 #get 00z forecast if we are before 12z, else get 12z forecast\n",
    "current_date = datetime.now().date() #select current date forecast\n",
    "var = '2t' #forecast returned variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdd195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To ensure the stability of our systems and to preserve resources for our operational activities (network, compute, etc.), access to the open-data portal is limited to 500 simultaneous connections. This limit helps us guarantee reliable service for our operational users, especially during periods of high demand. For added reliability, the open-data is replicated across AWS, Azure, and Google Cloud. If you experience difficulties accessing the portal directly, you can also retrieve the data from these cloud platforms.\n",
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By downloading data from the ECMWF open data dataset, you agree to the terms: Attribution 4.0 International (CC BY 4.0). Please attribute ECMWF when downloading this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#Download forecast data\n",
    "client = Client()\n",
    "result = client.retrieve(\n",
    "    date=current_date, #can be 0 for today, -1 yestarday... if \n",
    "    time=time,\n",
    "    type=\"fc\",  #forecast for HRES\n",
    "    stream='oper',\n",
    "    step=steps,\n",
    "    param=var,\n",
    "    target=\"current.grib2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a6b73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'current.grib2.5b7b6.idx' older than GRIB file\n"
     ]
    }
   ],
   "source": [
    "#Open the forecast dataset and format the datas\n",
    "ds = xr.open_dataset('current.grib2')\n",
    "forecast_date = ds.time.values #keep track of the forecast run date time\n",
    "us = ds.sel(**{\"latitude\": slice(50, 24), \"longitude\": slice(-125, -67)}) #Slice to get only US \n",
    "us = (us - 273.15) * 1.8 + 32 #Convert kelvin to 째F\n",
    "us.attrs['units'] = '째F'\n",
    "\n",
    "us = us.swap_dims({\"step\": \"valid_time\"})\n",
    "\n",
    "#We slice the dataset with valid_times to avoid non complete day between 00z run (complete days) and 12z run (non complete days)\n",
    "us = us.sel(valid_time=slice(pd.Timestamp(pd.Timestamp(ds.time.values).date()) + pd.Timedelta(days=1), #set the dataset at the start of the next day for our first window\n",
    "                             pd.Timestamp(pd.Timestamp(ds.time.values).date()) + pd.Timedelta(days=14, hours=23))) #and just before the last valid_time\n",
    "\n",
    "#Now that we start the first day at 00z for 00z run AND 12z run, resample hourly to daily\n",
    "us_daily = us.resample(valid_time=\"1D\").mean()\n",
    "\n",
    "us_daily_hdd = (65 - us_daily).clip(min=0) #compute HDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8370788",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = xr.open_dataarray('../utils/files/population_regridded_025deg.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f2e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdd_weighted = us_daily_hdd * pop #Weight the hdd by population for each point in the grid and each valid_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb54f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [(0,2, 'Day 1-3'), (3,6, 'Day 4-7'), (7,13, 'Day 8-14')]\n",
    "hdd_list = []\n",
    "for horizon in horizons:\n",
    "    #Create horizon date for slicing\n",
    "    first_date = hdd_weighted.valid_time.min().values\n",
    "    start_date = first_date + pd.Timedelta(days=horizon[0])\n",
    "    last_date = first_date + pd.Timedelta(days=horizon[1])\n",
    "\n",
    "    hdd_weighted_horizon = hdd_weighted.sel(valid_time=slice(start_date, last_date)).sum(dim='valid_time') #sumed HDD for every grid point in the horizon\n",
    "\n",
    "    #Make US mean\n",
    "    us_horizon_mean = hdd_weighted_horizon.mean(dim=['latitude', 'longitude']) #Mean every point in the US to have one mean weighted HDD for the horizon\n",
    "    hdd_list.append({'forecast_run_time': forecast_date, 'region': 'US Mean', 'horizon_start': start_date, 'horizon_end': last_date,\n",
    "                    'horizon_label': horizon[2], 'forecast_HDD': us_horizon_mean.t2m.item()})\n",
    "    \n",
    "    #Make region means\n",
    "    zone_means = regions_from_xarray(hdd_weighted_horizon)\n",
    "\n",
    "    #For each zone in the horizon, make a new row of data\n",
    "    for zone_name, zone_data in zone_means.items():\n",
    "        hdd_list.append({'forecast_run_time': forecast_date, 'region': zone_name, 'horizon_start': start_date, 'horizon_end': last_date,\n",
    "                         'horizon_label': horizon[2], 'forecast_HDD': zone_data.t2m.item()})\n",
    "        \n",
    "#Make a dataframe of values\n",
    "hdd_horizon_df = pd.DataFrame(hdd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b3394d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_run_time</th>\n",
       "      <th>region</th>\n",
       "      <th>horizon_start</th>\n",
       "      <th>horizon_end</th>\n",
       "      <th>horizon_label</th>\n",
       "      <th>forecast_HDD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>US Mean</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>1.779145e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>Great Lakes</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>4.983731e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>Columbia-Pacific Northwest</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>6.902799e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>Missouri Basin</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>4.797711e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>North Atlantic-Appalachian</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>9.825774e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  forecast_run_time                      region horizon_start horizon_end  \\\n",
       "0        2026-01-30                     US Mean    2026-01-31  2026-02-02   \n",
       "1        2026-01-30                 Great Lakes    2026-01-31  2026-02-02   \n",
       "2        2026-01-30  Columbia-Pacific Northwest    2026-01-31  2026-02-02   \n",
       "3        2026-01-30              Missouri Basin    2026-01-31  2026-02-02   \n",
       "4        2026-01-30  North Atlantic-Appalachian    2026-01-31  2026-02-02   \n",
       "\n",
       "  horizon_label  forecast_HDD  \n",
       "0       Day 1-3  1.779145e+06  \n",
       "1       Day 1-3  4.983731e+06  \n",
       "2       Day 1-3  6.902799e+05  \n",
       "3       Day 1-3  4.797711e+05  \n",
       "4       Day 1-3  9.825774e+06  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdd_horizon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea059212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43593b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd574741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25e70504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forcast_hdd(filepath, horizons):\n",
    "    #Open the forecast dataset and format the datas\n",
    "    ds = xr.open_dataset(filepath)\n",
    "    forecast_date = ds.time.values #keep track of the forecast run date time\n",
    "    us = ds.sel(**{\"latitude\": slice(50, 24), \"longitude\": slice(-125, -67)}) #Slice to get only US \n",
    "    us = (us - 273.15) * 1.8 + 32 #Convert kelvin to 째F\n",
    "    us.attrs['units'] = '째F'\n",
    "\n",
    "    us = us.swap_dims({\"step\": \"valid_time\"})\n",
    "\n",
    "    #We slice the dataset with valid_times to avoid non complete day between 00z run (complete days) and 12z run (non complete days)\n",
    "    us = us.sel(valid_time=slice(pd.Timestamp(pd.Timestamp(ds.time.values).date()) + pd.Timedelta(days=1), #set the dataset at the start of the next day for our first window\n",
    "                                pd.Timestamp(pd.Timestamp(ds.time.values).date()) + pd.Timedelta(days=14, hours=23))) #and just before the last valid_time\n",
    "\n",
    "    #Now that we start the first day at 00z for 00z run AND 12z run, resample hourly to daily\n",
    "    us_daily = us.resample(valid_time=\"1D\").mean()\n",
    "\n",
    "    us_daily_hdd = (65 - us_daily).clip(min=0) #compute HDD\n",
    "\n",
    "    #Open population file reggrided to weather forecasts\n",
    "    pop = xr.open_dataarray('../utils/files/population_regridded_025deg.nc')\n",
    "\n",
    "    hdd_weighted = us_daily_hdd * pop #Weight the hdd by population for each point in the grid and each valid_time\n",
    "\n",
    "    #Compute HDD for every horizons\n",
    "    hdd_list = []\n",
    "    for horizon in horizons:\n",
    "        #Create horizon date for slicing\n",
    "        start_date = min(horizon[0:2])\n",
    "        last_date = max(horizon[0:2])\n",
    "\n",
    "        hdd_weighted_horizon = hdd_weighted.sel(valid_time=slice(start_date, last_date)).sum(dim='valid_time') #sumed HDD for every grid point in the horizon\n",
    "\n",
    "        #Make US mean\n",
    "        us_horizon_mean = hdd_weighted_horizon.mean(dim=['latitude', 'longitude']) #Mean every point in the US to have one mean weighted HDD for the horizon\n",
    "        hdd_list.append({'forecast_run_time': forecast_date, 'region': 'US Mean', 'horizon_start': pd.Timestamp(start_date), 'horizon_end': pd.Timestamp(last_date),\n",
    "                        'horizon_label': horizon[2], 'forecast_HDD': us_horizon_mean.t2m.item()})\n",
    "        \n",
    "        #Make region means\n",
    "        zone_means = regions_from_xarray(hdd_weighted_horizon)\n",
    "\n",
    "        #For each zone in the horizon, make a new row of data\n",
    "        for zone_name, zone_data in zone_means.items():\n",
    "            hdd_list.append({'forecast_run_time': forecast_date, 'region': zone_name, 'horizon_start': pd.Timestamp(start_date), 'horizon_end': pd.Timestamp(last_date),\n",
    "                            'horizon_label': horizon[2], 'forecast_HDD': zone_data.t2m.item()})\n",
    "            \n",
    "    #Make a dataframe of values\n",
    "    hdd_horizon_df = pd.DataFrame(hdd_list)\n",
    "\n",
    "    return hdd_horizon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e86be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_vs_forecast(filepath, base_filepath, horizons):\n",
    "    #Load and format base HDD file\n",
    "    us_base_hdd = pd.read_csv(base_filepath)\n",
    "    us_base_hdd = us_base_hdd.rename(columns={'Unnamed: 0': 'doy'})\n",
    "    us_base_hdd = us_base_hdd.set_index('doy')\n",
    "\n",
    "    #Compute HDD from forecast file\n",
    "    hdd_forecast = compute_forcast_hdd(filepath, horizons)\n",
    "\n",
    "    #Add day of year columns to prepare the sum of base days\n",
    "    hdd_forecast['doy_horizon_start'] = hdd_forecast['horizon_start'].dt.day_of_year\n",
    "    hdd_forecast['doy_horizon_end'] = hdd_forecast['horizon_end'].dt.day_of_year\n",
    "\n",
    "    #Prepare a list of horizon days with horizon label\n",
    "    horizon_doy = list(set([(start, end, label) for start, end, label in zip(hdd_forecast['doy_horizon_start'].values, \n",
    "                                                                             hdd_forecast['doy_horizon_end'].values, \n",
    "                                                                             hdd_forecast['horizon_label'].values)]))\n",
    "    \n",
    "    #Sum the mean base hdd for each day in each horzion and each region\n",
    "    hdd_horizon_list = []\n",
    "    for hd in horizon_doy:\n",
    "        us_base_hdd_horizon = us_base_hdd.loc[hd[0]:hd[1]]\n",
    "        summed_base_hdd_horizon = pd.DataFrame(us_base_hdd_horizon.T.sum(axis=1).reset_index())\n",
    "        summed_base_hdd_horizon = summed_base_hdd_horizon.rename(columns={'index': 'region', 0: 'sum_base_HDD'})\n",
    "        summed_base_hdd_horizon['horizon_label'] = hd[2]\n",
    "        hdd_horizon_list.append(summed_base_hdd_horizon)\n",
    "    full_hdd_base_horizon = pd.concat(hdd_horizon_list)\n",
    "\n",
    "    #Merge the sum base hdd and the forcast hdd on each region and each horizon\n",
    "    hdd_forecast_base = pd.merge(hdd_forecast, full_hdd_base_horizon, how='inner', on=['region', 'horizon_label'])\n",
    "    hdd_forecast_base['delta_forecast_base'] = hdd_forecast_base['forecast_HDD'] - hdd_forecast_base['sum_base_HDD'] #delta between forecast and base\n",
    "\n",
    "    return hdd_forecast_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe79b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.mongo import MongoWrapper\n",
    "from pipeline.config import settings\n",
    "\n",
    "def forecast_vs_forecast(current_forecast):\n",
    "        #Get current forcast date from base_vs_forecast compute\n",
    "        current_forecast_time = current_forecast['forecast_run_time'].unique()[0]\n",
    "        previous_forecast_time = current_forecast_time - pd.Timedelta(hours=12)\n",
    "\n",
    "        #Database connection\n",
    "        mongo = MongoWrapper(settings.MONGO_URI, settings.MONGO_DB)\n",
    "        db = mongo.collection(settings.MONGO_COLLECTION)\n",
    "        \n",
    "        #Find previous forecasts in db, continue only if not empty (meaning we have a previous forecast that match)\n",
    "        query_res = db.find({'forecast_run_time': previous_forecast_time})\n",
    "        previous_hdd = pd.DataFrame(list(query_res))\n",
    "        if not previous_hdd.empty:\n",
    "            previous_hdd = previous_hdd.drop('_id', axis=1)\n",
    "            prev_forecast_horizon = list(set([(pd.Timestamp(start), pd.Timestamp(end), label) for start, end, label in zip(previous_hdd['horizon_start'].values, \n",
    "                                                                                    previous_hdd['horizon_end'].values, \n",
    "                                                                                    previous_hdd['horizon_label'].values)]))\n",
    "            \n",
    "        #Compute current forecast file with previous forecast time horizon\n",
    "        current_hdd_with_prev_horizon_df = compute_forcast_hdd('current.grib2', prev_forecast_horizon)\n",
    "\n",
    "        #Make sure that we have the same number of rows in both previous and current forecasts\n",
    "        assert len(previous_hdd) == len(current_hdd_with_prev_horizon_df)\n",
    "\n",
    "        #Format and merge and compute current - previous forecast\n",
    "        previous_hdd = previous_hdd.rename(columns={'forecast_run_time': 'prev_forecast_run_time', 'forecast_HDD': 'prev_forecast_HDD'})\n",
    "        current_and_prev_hdd = pd.merge(previous_hdd, current_hdd_with_prev_horizon_df, how='inner', on=['region', 'horizon_start', 'horizon_end', 'horizon_label'])\n",
    "        current_and_prev_hdd['delta_current_forecast_to_prev_forecast'] = current_and_prev_hdd['forecast_HDD'] - current_and_prev_hdd['prev_forecast_HDD']\n",
    "        delta_forecast_to_forecast = current_and_prev_hdd[['forecast_run_time', 'region', 'horizon_start', 'horizon_end', 'horizon_label', 'delta_current_forecast_to_prev_forecast']]\n",
    "        delta_forecast_to_forecast = delta_forecast_to_forecast.rename(columns={'horizon_start': 'prev_horizon_start', 'horizon_end': 'prev_horizon_end'})\n",
    "\n",
    "        return delta_forecast_to_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fae100cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forecast(base_forecast_data, forecast_vs_forecast_data):\n",
    "    full_hdd = pd.merge(base_forecast_data, forecast_vs_forecast_data, how='inner', on=['forecast_run_time', 'region', 'horizon_label']).reset_index(drop=True)\n",
    "    return full_hdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41d37b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "data_date = result.datetime\n",
    "horizons = [(data_date.date() + timedelta(days=1), data_date.date() + timedelta(days=3), 'Day 1-3'),\n",
    "            (data_date.date() + timedelta(days=4), data_date.date() + timedelta(days=7), 'Day 4-7'),\n",
    "            (data_date.date() + timedelta(days=8), data_date.date() + timedelta(days=14), 'Day 8-14')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96cfb027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file 'current.grib2.5b7b6.idx' older than GRIB file\n",
      "Ignoring index file 'current.grib2.5b7b6.idx' older than GRIB file\n"
     ]
    }
   ],
   "source": [
    "base_and_forecast = base_vs_forecast('current.grib2', '../utils/files/base.csv', horizons)\n",
    "forecast_and_forecast = forecast_vs_forecast(base_and_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6077fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdd = full_forecast(base_and_forecast, forecast_and_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c217c0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_run_time</th>\n",
       "      <th>region</th>\n",
       "      <th>horizon_start</th>\n",
       "      <th>horizon_end</th>\n",
       "      <th>horizon_label</th>\n",
       "      <th>forecast_HDD</th>\n",
       "      <th>doy_horizon_start</th>\n",
       "      <th>doy_horizon_end</th>\n",
       "      <th>sum_base_HDD</th>\n",
       "      <th>delta_forecast_base</th>\n",
       "      <th>prev_horizon_start</th>\n",
       "      <th>prev_horizon_end</th>\n",
       "      <th>delta_current_forecast_to_prev_forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>US Mean</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>1.779145e+06</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>2.875859e+05</td>\n",
       "      <td>1.491559e+06</td>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>-6.363584e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>Great Lakes</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>4.983731e+06</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>5.842958e+05</td>\n",
       "      <td>4.399436e+06</td>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>-2.025188e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>Columbia-Pacific Northwest</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>6.902799e+05</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>1.446409e+05</td>\n",
       "      <td>5.456390e+05</td>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>-2.617819e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>Missouri Basin</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>4.797711e+05</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>7.000968e+04</td>\n",
       "      <td>4.097615e+05</td>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>-1.914042e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>North Atlantic-Appalachian</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-02-02</td>\n",
       "      <td>Day 1-3</td>\n",
       "      <td>9.825774e+06</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>1.024323e+06</td>\n",
       "      <td>8.801451e+06</td>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>-3.495451e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  forecast_run_time                      region horizon_start horizon_end  \\\n",
       "0        2026-01-30                     US Mean    2026-01-31  2026-02-02   \n",
       "1        2026-01-30                 Great Lakes    2026-01-31  2026-02-02   \n",
       "2        2026-01-30  Columbia-Pacific Northwest    2026-01-31  2026-02-02   \n",
       "3        2026-01-30              Missouri Basin    2026-01-31  2026-02-02   \n",
       "4        2026-01-30  North Atlantic-Appalachian    2026-01-31  2026-02-02   \n",
       "\n",
       "  horizon_label  forecast_HDD  doy_horizon_start  doy_horizon_end  \\\n",
       "0       Day 1-3  1.779145e+06                 31               33   \n",
       "1       Day 1-3  4.983731e+06                 31               33   \n",
       "2       Day 1-3  6.902799e+05                 31               33   \n",
       "3       Day 1-3  4.797711e+05                 31               33   \n",
       "4       Day 1-3  9.825774e+06                 31               33   \n",
       "\n",
       "   sum_base_HDD  delta_forecast_base prev_horizon_start prev_horizon_end  \\\n",
       "0  2.875859e+05         1.491559e+06         2026-01-30       2026-02-01   \n",
       "1  5.842958e+05         4.399436e+06         2026-01-30       2026-02-01   \n",
       "2  1.446409e+05         5.456390e+05         2026-01-30       2026-02-01   \n",
       "3  7.000968e+04         4.097615e+05         2026-01-30       2026-02-01   \n",
       "4  1.024323e+06         8.801451e+06         2026-01-30       2026-02-01   \n",
       "\n",
       "   delta_current_forecast_to_prev_forecast  \n",
       "0                            -6.363584e+05  \n",
       "1                            -2.025188e+06  \n",
       "2                            -2.617819e+05  \n",
       "3                            -1.914042e+05  \n",
       "4                            -3.495451e+06  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hdd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d6f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
