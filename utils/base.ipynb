{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712ac3f7",
   "metadata": {},
   "source": [
    "Make historical HDD mean (base) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f68ca394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db9a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regions_from_xarray(hdd_dataset, pop_dataset):\n",
    "    #Load subregion shapefile\n",
    "    gdf = gpd.read_file('../utils/files/USRegion/12Regions/DOI_12_Unified_Regions_20180801.shp')\n",
    "    gdf = gdf.to_crs(epsg=4326) #reproject file into classical lat lon coordinates\n",
    "    gdf = gdf[['REG_NAME', 'geometry']]\n",
    "    gdf = gdf[~gdf['REG_NAME'].isin(['Alaska', 'Pacific Islands'])] #Remove both of these regions, irrelevant\n",
    "\n",
    "    #Make a point grid from dataset\n",
    "    lon_grid, lat_grid = np.meshgrid(hdd_dataset.longitude.values, hdd_dataset.latitude.values)\n",
    "    coords_flat = list(zip(lon_grid.ravel(), lat_grid.ravel()))\n",
    "\n",
    "    #Make a gdf of dataset grid to prepare for spatial join\n",
    "    gdf_points = gpd.GeoDataFrame({\n",
    "        'latitude': lat_grid.flatten(),\n",
    "        'longitude': lon_grid.flatten(),\n",
    "        'geometry': gpd.points_from_xy([c[0] for c in coords_flat], [c[1] for c in coords_flat])\n",
    "    }, crs=gdf.crs) #Make sure we have the same crs\n",
    "\n",
    "    #Spatial join between the full US grid and the subregions\n",
    "    gdf_joined = gpd.sjoin(gdf_points, gdf[['REG_NAME', 'geometry']], how='inner', predicate='within')\n",
    "\n",
    "    #Find subregions points and mean then to have the mean of a region for a day of year\n",
    "    zone_means = {}\n",
    "    for zone in gdf_joined['REG_NAME'].unique():\n",
    "        zone_pts = gdf_joined[gdf_joined['REG_NAME'] == zone]\n",
    "        zone_data = hdd_dataset.sel(latitude=xr.DataArray(zone_pts['latitude'].values, dims='points'), #Select region points from the xarray dataset\n",
    "                                    longitude=xr.DataArray(zone_pts['longitude'].values, dims='points'),\n",
    "                                    method='nearest')\n",
    "        zone_pop = pop_dataset.sel(latitude=xr.DataArray(zone_pts['latitude'].values, dims='points'), #Select region points from the xarray dataset\n",
    "                                   longitude=xr.DataArray(zone_pts['longitude'].values, dims='points'),\n",
    "                                   method='nearest')\n",
    "        zone_hdd_sum = zone_data.sum(dim='points') #sum the founded points HDD\n",
    "        zone_pop_sum = zone_pop.sum(dim='points') #sum the pop\n",
    "\n",
    "        zone_hdd_sum_pop_weighted = zone_hdd_sum / zone_pop_sum\n",
    "        \n",
    "        zone_means[zone] = zone_hdd_sum_pop_weighted\n",
    "\n",
    "    return zone_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a42d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexl\\miniconda3\\envs\\geo\\Lib\\site-packages\\zarr\\codecs\\numcodecs\\_codecs.py:163: ZarrUserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n"
     ]
    }
   ],
   "source": [
    "#Read Zarr dataset from earthdatahub destine EU\n",
    "ds = xr.open_dataset(\n",
    "    \"https://data.earthdatahub.destine.eu/era5/era5-land-daily-utc-v1.zarr\",\n",
    "    storage_options={\"client_kwargs\":{\"trust_env\":True}},\n",
    "    chunks={},\n",
    "    engine=\"zarr\",\n",
    ")\n",
    "\n",
    "#Reassign longitude to be -180, 180\n",
    "if ds.longitude.max() > 180:\n",
    "    ds = ds.assign_coords(longitude=((ds.longitude + 180) % 360) - 180)\n",
    "    ds = ds.sortby(\"longitude\")\n",
    "ds_us = ds.sel(**{\"latitude\": slice(50, 24), \"longitude\": slice(-125, -67)}) #Slice to get only US \n",
    "\n",
    "#Format dataset for HDD\n",
    "t2m_us = ds_us.t2m #Keep only 2m temperature\n",
    "t2m_us = (t2m_us - 273.15) * 1.8 + 32 #Convert kelvin to °F\n",
    "t2m_us.attrs['units'] = '°F'\n",
    "t2m_30years_us = t2m_us.sel(valid_time=slice('1995', '2025')) #Select last 30 years\n",
    "\n",
    "hdd = (65 - t2m_30years_us).clip(min=0) #make max(0, 65 - t2m)\n",
    "\n",
    "#Population reggrided to ERA5-Land dataset\n",
    "pop = xr.open_dataarray('../utils/files/population_regridded_01deg.nc') #from reproject_and_align_pop function in tools\n",
    "\n",
    "hdd_weighted = hdd * pop #Weight the hdd by population for each point in the grid and each valid_time\n",
    "\n",
    "hdd_weighted_yearly_mean = hdd_weighted.groupby(hdd_weighted.valid_time.dt.dayofyear).mean() #Make the mean of each grid point for each day of year (1-365)\n",
    "hdd_weighted_yearly_mean_computed = hdd_weighted_yearly_mean.compute() #here we finally compute the xarray because that's the one we will then manipulate for US and subregion mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34cd78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make US sum\n",
    "us_yearly_sum = hdd_weighted_yearly_mean_computed.sum(dim=['latitude', 'longitude']) #Mean every point in the US to have one mean weighted HDD for each day of year\n",
    "us_pop_sum = pop.sum(dim=['latitude', 'longitude'])\n",
    "base_us_weighted_hdd = us_yearly_sum / us_pop_sum\n",
    "base_us_weighted_hdd_df = base_us_weighted_hdd.to_dataframe(name='US Sum')\n",
    "base_us_weighted_hdd_df = base_us_weighted_hdd_df[['US Sum']]\n",
    "\n",
    "#Make subregion sum\n",
    "zone_means = regions_from_xarray(hdd_weighted_yearly_mean_computed, pop)\n",
    "\n",
    "#Make a dataframe from mean dict with day of year as index and regions as columns\n",
    "subregion_yearly_mean_df = pd.DataFrame({zone: data.values for zone, data in zone_means.items()}, index=hdd_weighted_yearly_mean_computed.dayofyear.values)\n",
    "\n",
    "us_base_hdd = pd.concat([base_us_weighted_hdd_df, subregion_yearly_mean_df], axis=1) #add US Mean to a new column\n",
    "\n",
    "if 366 in us_base_hdd.index: #if 366 days in a year, we remove the last one (366th)\n",
    "    us_base_hdd = us_base_hdd.drop(index=366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12198dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_base_hdd.to_csv('../utils/files/base_revised.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
